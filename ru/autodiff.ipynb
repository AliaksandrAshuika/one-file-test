{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Введение в градиенты и автоматическое дифференцирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> Посмотреть на TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Запустить в Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Посмотреть источник на GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\"> Скачать блокнот</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Автоматическое дифференцирование и градиенты\n",
        "\n",
        "[Автоматическое дифференцирование](https://en.wikipedia.org/wiki/Automatic_differentiation) полезно для реализации алгоритмов машинного обучения, таких как [обратное распространение](https://en.wikipedia.org/wiki/Backpropagation) для обучения нейронных сетей.\n",
        "\n",
        "В этом руководстве мы обсудим способы вычисления градиентов с помощью TensorFlow, особенно в стремительном исполнении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Настроить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Вычислительные градиенты\n",
        "\n",
        "Для автоматической дифференциации TensorFlow необходимо запомнить, какие операции происходят в каком порядке во время *прямого* прохода. Затем во время *обратного прохода* TensorFlow перебирает этот список операций в обратном порядке для вычисления градиентов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Градиентные ленты\n",
        "\n",
        "TensorFlow предоставляет API [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) для автоматической дифференциации; то есть вычисление градиента вычисления относительно его входных переменных. TensorFlow «записывает» все операции, выполненные в контексте `tf.GradientTape` на «ленту». Затем TensorFlow использует эту ленту и градиенты, связанные с каждой записанной операцией, для вычисления градиентов «записанного» вычисления с использованием [дифференцирования в обратном режиме](https://en.wikipedia.org/wiki/Automatic_differentiation) .\n",
        "\n",
        "Со скалярами:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [],
      "source": [
        "x = tf.constant(3.0)\n",
        "# y = x ^ 2\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(x)\n",
        "  y = x * x\n",
        "# dy = 2x\n",
        "dy_dx = t.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jen-yWFxIAWQ"
      },
      "source": [
        "Используя матрицы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bAFeIE8EuVIq"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([3.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(x)\n",
        "  z = tf.multiply(x, x)\n",
        "\n",
        "print(z)\n",
        "\n",
        "# Find derivative of z with respect to the original input tensor x\n",
        "print(t.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "Все операции на `tf.Variable` добавляются на ленту. Чтобы записать градиенты относительно входных данных с постоянным тензором (как указано выше), вам нужно добавить `t.watch(x)` чтобы лента градиента отслеживала входной тензор.\n",
        "\n",
        "Note: A common problem is that inputs are not `tf.Tensors`, but some tensor-like structure like a NumPy `ndarray` or a Python list.  This kind of input will be converted to tensors within the first op, but do not start as tensors, which may result in confusing `None` gradients or errors because you have accidentally passed in integers (which cannot form gradients).  To forestall this issue, use `tf.constant` with an appropriate `dtype`, or `tf.data.from_tensor_slices` to convert your input data into tensors or a `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "### Промежуточные результаты\n",
        "\n",
        "Вы также можете запросить градиенты вывода по отношению к промежуточным значениям, вычисленным во время «записанного» контекста `tf.GradientTape` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([3.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  t.watch(x)\n",
        "  y = tf.multiply(x, x)\n",
        "  z = tf.multiply(y, y)\n",
        "\n",
        "# Use the tape to compute the derivative of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dy = 2 * y, where y = x ^ 2\n",
        "print(t.gradient(z, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "Градиентные ленты автоматически просматривают любую переменную, к которой обращаются в пределах их видимости. Вы можете перечислить их в порядке их создания."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3AhiLRS8N7QY"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  # Don't need any calls to watch(), as when variables x and y\n",
        "  # get used, they will be automatically watched\n",
        "  y_sq = y * y\n",
        "  z = x + y_sq\n",
        "\n",
        "t.watched_variables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "By default, the resources held by a `GradientTape` are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a `persistent` gradient tape. This allows multiple calls to the `gradient()` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "print(t.gradient(z, x))  # 108.0 (4*x^3 at x = 3)\n",
        "print(t.gradient(y, x))  # 6.0 (2 * x)\n",
        "del t  # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "### Примечания по производительности\n",
        "\n",
        "- Есть небольшие накладные расходы, связанные с выполнением операций в контексте градиентной ленты. Для большинства стремящихся к исполнению это не будет заметной стоимостью, но вы все равно должны использовать контекст ленты только в тех областях, где это требуется.\n",
        "\n",
        "- Градиентные ленты используют память для хранения промежуточных результатов, включая входы и выходы, для использования во время обратного прохода.\n",
        "\n",
        "    Для эффективности некоторым операциям (например, `ReLU` ) не нужно сохранять промежуточные результаты, и они сокращаются во время прямого прохода. Однако, если вы используете `persistent=True` на вашей ленте, *ничего не удаляется,* и ваше пиковое использование памяти будет выше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "### Запись управления потоком\n",
        "\n",
        "Поскольку ленты записывают операции по мере их выполнения, поток управления Python (с использованием `if` s и `while` s) обрабатывается естественным образом:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9FViq92UX7P8"
      },
      "outputs": [],
      "source": [
        "def f(x, y):\n",
        "  output = 1.0\n",
        "  for i in range(y):\n",
        "    if i > 1 and i < 5:\n",
        "      output *= x  # tf.multiply(output, x)\n",
        "  return output\n",
        "\n",
        "def grad(x, y):\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch(x)\n",
        "    out = f(x, y)\n",
        "  return t.gradient(out, x)\n",
        "\n",
        "x = tf.constant(2.0)\n",
        "\n",
        "print(grad(x, 6).numpy())  # 12.0\n",
        "print(grad(x, 5).numpy())  # 12.0\n",
        "print(grad(x, 4).numpy())  # 4.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "### Получение градиента `None`\n",
        "\n",
        "Когда градиент недоступен по какой-либо причине, вы обычно получаете градиент `None` .\n",
        "\n",
        "Распространенной ситуацией является попытка получить градиент не плавая и не сложного значения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [],
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  x = tf.Variable([[2, 2], [2, 2]], dtype=tf.int8)\n",
        "  y = x * x\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4PVxPRC2AS5p"
      },
      "source": [
        "In other situations, you may be doing operations that are not differentiable. `DecodeJpeg`, for example, does not produce a gradient.  To see which operations have gradients registered for them, see the list of [raw ops](https://www.tensorflow.org/api_docs/python/tf/raw_ops).\n",
        "\n",
        "Наконец, вы можете просто получать градиенты, где нет никакой связи между входом и выходом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "Может быть удобно вернуть 0 вместо `None` . Вы можете решить, что возвращать, когда у вас есть неподключенные градиенты, используя аргумент `unconnected_gradient` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(x, y, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mbb-9lnGVngH"
      },
      "source": [
        "## Пользовательские градиенты\n",
        "\n",
        "В некоторых случаях вы можете захотеть контролировать, как именно рассчитываются градиенты, а не использовать значения по умолчанию. Эти ситуации включают в себя:\n",
        "\n",
        "- Там нет определенного градиента для новой операции, которую вы пишете.\n",
        "- Расчеты по умолчанию численно нестабильны.\n",
        "- Вы хотите кэшировать дорогостоящие вычисления с прямого прохода.\n",
        "- Вы хотите изменить значение (например, используя: `tf.clip_by_value` , `tf.math.round` ) без изменения градиента.\n",
        "\n",
        "Для написания новой операции вы можете использовать `tf.RegisterGradient` чтобы создать свою собственную. Смотрите эту страницу для деталей. (Обратите внимание, что реестр градиентов является глобальным, поэтому изменяйте его с осторожностью.)\n",
        "\n",
        "Для последних двух случаев вы можете использовать `tf.custom_gradient` . Вот пример, который применяет `tf.clip_by_norm` к градиенту.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Mjj01w4NYtwd"
      },
      "outputs": [],
      "source": [
        "# Establish an identity operation, but clip during the gradient pass\n",
        "@tf.custom_gradient\n",
        "def clip_gradients(y):\n",
        "  def backward(dy):\n",
        "    return tf.clip_by_norm(dy, 0.5)\n",
        "  return y, backward\n",
        "\n",
        "v = tf.Variable(2.0)\n",
        "with tf.GradientTape() as t:\n",
        "  output = clip_gradients(v * v)\n",
        "print(t.gradient(output, v))  # calls \"backward\", which clips 4 to 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n4t7S0scYrD3"
      },
      "source": [
        "Смотрите декоратор `tf.custom_gradient` для более подробной информации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DK05KXrAAld3"
      },
      "source": [
        "### Градиенты высших порядков\n",
        "\n",
        "Операции внутри контекстного менеджера `GradientTape` записываются для автоматической дифференциации. Если градиенты вычисляются в этом контексте, то вычисление градиента также записывается. В результате точно такой же API работает и для градиентов более высокого порядка. Например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cPQgthZ7ugRJ"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  with tf.GradientTape() as t2:\n",
        "    y = x * x * x\n",
        "\n",
        "  # Compute the gradient inside the 't' context manager\n",
        "  # which means the gradient computation is differentiable as well.\n",
        "  dy_dx = t2.gradient(y, x)\n",
        "d2y_dx2 = t.gradient(dy_dx, x)\n",
        "\n",
        "print(\"dy_dx:\", dy_dx.numpy())  # 3.0\n",
        "print(\"d2y_dx2:\", d2y_dx2.numpy())  # 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fc-PocLB-TP3"
      },
      "source": [
        "## Расширенные функции для `GradientTape`\n",
        "\n",
        "В этом разделе мы рассмотрим некоторые менее распространенные способы использования `GradientTape` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uGRJJRi8TCkJ"
      },
      "source": [
        "### Управление градиентной записью\n",
        "\n",
        "Вы можете выбрать просмотр только отдельных переменных. Для этого вы можете отключить просмотр всех переменных по умолчанию. В этом случае вам нужно будет просмотреть все переменные вручную."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "KT6E-xYUKpGN"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False, persistent=True) as t:\n",
        "  # Only watch y\n",
        "  t.watch(y)\n",
        "  y_sq = y * y\n",
        "  z = x + y_sq\n",
        "\n",
        "# Gradient will be None, as x is not watched, so the tape cannot\n",
        "# differentiate it\n",
        "print(\"Gradient with respect to x:\", t.gradient(z, x))\n",
        "# Gradient will be 6, as y is watched\n",
        "print(\"Gradient with respect to y:\", t.gradient(z, y))\n",
        "del t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gB_i0VnhQKt2"
      },
      "source": [
        "Если вы хотите остановить запись градиентов, вы можете использовать `stop_recording()` чтобы временно приостановить запись.\n",
        "\n",
        "Если вы хотите начать все сначала, используйте `reset()` . Простой выход из блока градиентной ленты и перезапуск обычно проще для чтения, но вы можете использовать `reset` если выход из блока ленты затруднен или невозможен."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mhFSYf7uQWxR"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(2.0)\n",
        "y = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  y_sq = y * y\n",
        "  # Stop recording so we can calculate an intermediate gradient\n",
        "  with t.stop_recording():\n",
        "    print(\"y with respect to y_sq:\", t.gradient(y_sq, y))  # Will be 1\n",
        "  z = x + y_sq\n",
        "  t.reset()\n",
        "\n",
        "print(\"z with respect to y after reset:\", t.gradient(z, y))  # None"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
