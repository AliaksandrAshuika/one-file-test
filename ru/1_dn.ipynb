{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "klGNgWREsvQv"
      },
      "source": [
        "##### Copyright 2018 Авторы TF-Агентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "nQnmcm0oI1Q-"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pmDI-h7cI0tI"
      },
      "source": [
        "# Обучите глубокую сеть Q с TF-агентами\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\"> Посмотреть на TensorFlow.org</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"> Запустить в Google Colab</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"> Посмотреть источник на GitHub</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\"> Скачать блокнот</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lsaQlK8fFQqH"
      },
      "source": [
        "## Введение\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cKOCZlhUgXVK"
      },
      "source": [
        "В этом примере показано, как обучить [агента DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) в среде Cartpole с использованием библиотеки TF-Agents.\n",
        "\n",
        "![Среда обитания](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\n",
        "\n",
        "Он проведет вас через все компоненты конвейера Reinforcement Learning (RL) для обучения, оценки и сбора данных.\n",
        "\n",
        "Чтобы запустить этот код в прямом эфире, нажмите ссылку «Запустить в Google Colab» выше.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Настроить"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kNrNXKI7bINP"
      },
      "source": [
        "Если вы не установили следующие зависимости, запустите:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "KEHR2Ui-lo8O"
      },
      "outputs": [

      ],
      "source": [
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip install 'gym==0.10.11'\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install 'pyglet==1.3.2'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "sMitx5qSgJk1"
      },
      "outputs": [

      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "J6HsdS5GbSjd"
      },
      "outputs": [

      ],
      "source": [
        "tf.compat.v1.enable_v2_behavior()\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "NspmzG4nP3b9"
      },
      "outputs": [

      ],
      "source": [
        "tf.version.VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HC1kNrOsLSIZ"
      },
      "outputs": [

      ],
      "source": [
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Окружающая среда\n",
        "\n",
        "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
        "\n",
        "Загрузите среду CartPole из набора OpenAI Gym. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "pYEz-S9gEv2-"
      },
      "outputs": [

      ],
      "source": [
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IIHYVBkuvPNw"
      },
      "source": [
        "Вы можете визуализировать эту среду, чтобы увидеть, как она выглядит. Свободно качающийся столб прикреплен к тележке. Цель состоит в том, чтобы переместить тележку вправо или влево, чтобы полюс был направлен вверх."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9_lskPOey18"
      },
      "source": [
        "Метод `environment.step` выполняет `action` в среде и возвращает кортеж `TimeStep` содержащий следующее наблюдение за средой и вознаграждение за действие.\n",
        "\n",
        "Метод `time_step_spec()` возвращает спецификацию для кортежа `TimeStep` . Его атрибут `observation` показывает форму наблюдений, типы данных и диапазоны допустимых значений. Атрибут `reward` показывает те же детали для вознаграждения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "exDv57iHfwQV"
      },
      "outputs": [

      ],
      "source": [
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "UxiSyCbBUQPi"
      },
      "outputs": [

      ],
      "source": [
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b_lHcIcqUaqB"
      },
      "source": [
        "Метод `action_spec()` возвращает форму, типы данных и допустимые значения допустимых действий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "bttJ4uxZUQBr"
      },
      "outputs": [

      ],
      "source": [
        "print('Action Spec:')\n",
        "print(env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eJCgJnx3g0yY"
      },
      "source": [
        "В среде Cartpole:\n",
        "\n",
        "- `observation` представляет собой массив из 4 поплавков:\n",
        "    - положение и скорость тележки\n",
        "    - угловое положение и скорость полюса\n",
        "- `reward` является скалярным значением с плавающей запятой\n",
        "- `action` - это скалярное целое число, имеющее только два возможных значения:\n",
        "    - `0` - «двигаться влево»\n",
        "    - `1` - «двигаться вправо»\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "V2UGR5t_iZX-"
      },
      "outputs": [

      ],
      "source": [
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4JSc9GviWUBK"
      },
      "source": [
        "Обычно создаются две среды: одна для обучения и одна для оценки. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "N7brXNIGWXjC"
      },
      "outputs": [

      ],
      "source": [
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zuUqXAVmecTU"
      },
      "source": [
        "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
        "\n",
        "API исходной среды использует массивы Numpy. `TFPyEnvironment` преобразует их в `Tensors` чтобы сделать его совместимым с агентами и политиками Tensorflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [

      ],
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## агент\n",
        "\n",
        "Алгоритм, используемый для решения проблемы RL, представлен `Agent` . TF-Agents предоставляет стандартные реализации различных `Agents` , в том числе:\n",
        "\n",
        "- [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (используется в этом уроке)\n",
        "- [REINFORCE](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
        "- [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "- [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
        "- [РРО](https://arxiv.org/abs/1707.06347)\n",
        "- [SAC](https://arxiv.org/abs/1801.01290) .\n",
        "\n",
        "Агент DQN может использоваться в любой среде, которая имеет дискретное пространство действия.\n",
        "\n",
        "В основе агента DQN лежит `QNetwork` , модель нейронной сети, которая может научиться прогнозировать `QValues` (ожидаемые доходы) для всех действий, учитывая наблюдения из среды.\n",
        "\n",
        "Используйте `tf_agents.networks.q_network` для создания `QNetwork` , передавая `observation_spec` , `action_spec` и кортеж, описывающий количество и размер скрытых слоев модели.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "TgkdEPg_muzV"
      },
      "outputs": [

      ],
      "source": [
        "fc_layer_params = (100,)\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z62u55hSmviJ"
      },
      "source": [
        "Теперь используйте `tf_agents.agents.dqn.dqn_agent` для создания экземпляра `DqnAgent` . В дополнение к `time_step_spec` , `action_spec` и QNetwork для конструктора агента также требуется оптимизатор (в данном случае `AdamOptimizer` ), функция потерь и целочисленный счетчик шагов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [

      ],
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## полисы\n",
        "\n",
        "Политика определяет, как агент действует в среде. Как правило, целью обучения с подкреплением является обучение базовой модели, пока политика не даст желаемый результат.\n",
        "\n",
        "В этом уроке:\n",
        "\n",
        "- Желаемый результат - держать столб в вертикальном положении на тележке.\n",
        "- Политика возвращает действие (левое или правое) для каждого наблюдения `time_step` .\n",
        "\n",
        "Агенты содержат две политики:\n",
        "\n",
        "- `agent.policy` - основная политика, которая используется для оценки и развертывания.\n",
        "- `agent.collect_policy` - вторая политика, которая используется для сбора данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "BwY7StuMkuV4"
      },
      "outputs": [

      ],
      "source": [
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Qs1Fl3dV0ae"
      },
      "source": [
        "Политики могут быть созданы независимо от агентов. Например, используйте `tf_agents.policies.random_tf_policy` чтобы создать политику, которая будет случайным образом выбирать действие для каждого `time_step` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HE37-UCIrE69"
      },
      "outputs": [

      ],
      "source": [
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dOlnlRRsUbxP"
      },
      "source": [
        "Чтобы получить действие от политики, вызовите метод `policy.action(time_step)` . `time_step` содержит наблюдение из окружающей среды. Этот метод возвращает `PolicyStep` , который является именованным кортежем с тремя компонентами:\n",
        "\n",
        "- `action` - действие, которое нужно предпринять (в данном случае `0` или `1` )\n",
        "- `state` - используется для политик с состоянием (т. е. на основе RNN)\n",
        "- `info` - вспомогательные данные, такие как журнал вероятностей действий"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "5gCcpXswVAxk"
      },
      "outputs": [

      ],
      "source": [
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "D4DHZtq3Ndis"
      },
      "outputs": [

      ],
      "source": [
        "time_step = example_environment.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PRFqAUzpNaAW"
      },
      "outputs": [

      ],
      "source": [
        "random_policy.action(time_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Метрики и оценка\n",
        "\n",
        "Наиболее распространенным показателем, используемым для оценки политики, является средняя доходность. Возврат представляет собой сумму вознаграждений, полученных при выполнении политики в среде для эпизода. Несколько эпизодов запускаются, создавая средний доход.\n",
        "\n",
        "Следующая функция вычисляет среднюю доходность политики с учетом политики, среды и ряда эпизодов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "bitzHo5_UbXy"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_snCVvq5Z8lJ"
      },
      "source": [
        "Выполнение этого вычисления для `random_policy` показывает базовую производительность в среде."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9bgU6Q6BZ8Bp"
      },
      "outputs": [

      ],
      "source": [
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Буфер воспроизведения\n",
        "\n",
        "Буфер воспроизведения отслеживает данные, собранные из среды. В этом руководстве используется `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer` , так как он является наиболее распространенным.\n",
        "\n",
        "Конструктор требует спецификации для данных, которые он будет собирать. Это доступно от агента, используя метод `collect_data_spec` . Размер пакета и максимальная длина буфера также необходимы.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [

      ],
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZGNTDJpZs4NN"
      },
      "source": [
        "Для большинства агентов `collect_data_spec` - это именованный кортеж с именем `Trajectory` , содержащий спецификации для наблюдений, действий, наград и других предметов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "_IZ-3HcqgE1z"
      },
      "outputs": [

      ],
      "source": [
        "agent.collect_data_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "sy6g1tGcfRlw"
      },
      "outputs": [

      ],
      "source": [
        "agent.collect_data_spec._fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rVD5nQ9ZGo8_"
      },
      "source": [
        "## Сбор информации\n",
        "\n",
        "Теперь выполните случайную политику в среде за несколько шагов, записав данные в буфер воспроизведения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "wr1KSAEGG4h9"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
        "\n",
        "# This loop is so common in RL, that we provide standard implementations. \n",
        "# For more details see the drivers module.\n",
        "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84z5pQJdoKxo"
      },
      "source": [
        "Буфер воспроизведения теперь является коллекцией траекторий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "4wZnLu2ViO4E"
      },
      "outputs": [

      ],
      "source": [
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TujU-PMUsKjS"
      },
      "source": [
        "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
        "\n",
        "Каждая строка буфера воспроизведения хранит только один шаг наблюдения. Но поскольку агенту DQN требуется как текущее, так и следующее наблюдение для вычисления потерь, конвейер набора данных будет выбирать две соседние строки для каждого элемента в пакете ( `num_steps=2` ).\n",
        "\n",
        "Этот набор данных также оптимизирован за счет параллельных вызовов и предварительной выборки данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ba7bilizt_qW"
      },
      "outputs": [

      ],
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "K13AST-2ppOq"
      },
      "outputs": [

      ],
      "source": [
        "iterator = iter(dataset)\n",
        "\n",
        "print(iterator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Th5w5Sff0b16"
      },
      "outputs": [

      ],
      "source": [
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data \n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Обучение агента\n",
        "\n",
        "Во время цикла обучения должны произойти две вещи:\n",
        "\n",
        "- собирать данные из окружающей среды\n",
        "- использовать эти данные для обучения нейронной сети агента\n",
        "\n",
        "Этот пример также периодически оценивает политику и печатает текущий счет.\n",
        "\n",
        "Следующее займет ~ 5 минут для запуска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Визуализация\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Сюжеты\n",
        "\n",
        "Используйте `matplotlib.pyplot` чтобы составить график улучшения политики во время обучения.\n",
        "\n",
        "Одна итерация `Cartpole-v0` состоит из 200 временных шагов. Окружающая среда дает награду `+1` за каждый шаг, на котором находится полюс, поэтому максимальная отдача за один эпизод составляет 200. Диаграммы показывают увеличение отдачи до этого максимума при каждой оценке во время тренировки. (Это может быть немного нестабильно и не увеличиваться монотонно каждый раз.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "NxtL1mbOYCVO"
      },
      "outputs": [

      ],
      "source": [
        "#@test {\"skip\": true}\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Ролики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "Графики хорошие. Но более захватывающим является то, что агент действительно выполняет задачу в среде.\n",
        "\n",
        "Сначала создайте функцию для вставки видео в блокнот."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [

      ],
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "Теперь пройдитесь по нескольким эпизодам игры Cartpole с агентом. Базовая среда Python (та, что «внутри» оболочки среды TensorFlow) предоставляет метод `render()` , который выводит изображение состояния среды. Их можно собрать в видео."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "owOVWB158NlF"
      },
      "outputs": [

      ],
      "source": [
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "povaAOcZygLw"
      },
      "source": [
        "Для забавы сравните обученного агента (см. Выше) с агентом, движущимся случайным образом. (Это не так хорошо.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "pJZIdC37yNH4"
      },
      "outputs": [

      ],
      "source": [
        "create_policy_eval_video(random_policy, \"random-agent\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "DQN Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": [

        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
